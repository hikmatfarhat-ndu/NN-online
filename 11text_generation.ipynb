{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "ffa8138a954375a0eba8eca80543292cc4faeae39ef0340fcb1267261a1ca77f"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/NN-online/blob/main/11text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN\n",
        "Recurrent Neural Networks are typically used on __sequences__. Unlike the NNs we have seen so far an RNN takes a __sequence__ of inputs instead of just one. \n",
        "The main difference between RRNs and the others is that an RNN contains a __feedback__ loop as shown in the figure below.\n",
        "The figure also showns an __unrolled__ RNN when the input sequence is of size 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieZzscdrWmuP"
      },
      "source": [
        "![rnn](https://github.com/hikmatfarhat-ndu/NN-online/blob/main/figures/rnn.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.layers import SimpleRNN,SimpleRNNCell"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuYp0XX5WmuP"
      },
      "source": [
        "As we have seen above an RNN layer is basically a sequence of RNN cells that feed each other. To illustrate we compare the output from a SimpleRNN layer in Keras and the output of two SimpleRNN cells that feed one another"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rebqC7D2iPo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d2d948-091b-4e21-fe1c-63b0529000cb"
      },
      "source": [
        "## Initial state\n",
        "init=tf.zeros_like([[0,0,0,0]],dtype='float32')\n",
        "print(\"init shape={}\".format(init.shape))\n",
        "## the input contatins 1 single batch\n",
        "## of a sequence (size 2) of vectors of dim 3\n",
        "input=np.array([[1,1,1],[4,4,4]]).astype('float32')\n",
        "input=input.reshape(1,2,3)\n",
        "print(\"input shape={}\".format(input.shape))\n",
        "## Create a \"deterministic\" RNN so that we always\n",
        "## get the same output\n",
        "rnn=SimpleRNN(4,activation='linear', \n",
        "       kernel_initializer=tf.keras.initializers.Constant(1), \n",
        "        return_sequences=True,\n",
        "        recurrent_initializer=tf.keras.initializers.Constant(1)\n",
        "        )\n",
        "output=rnn(input,initial_state=init)\n",
        "print(np.squeeze(output.numpy()))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init shape=(1, 4)\n",
            "input shape=(1, 2, 3)\n",
            "[[ 3.  3.  3.  3.]\n",
            " [24. 24. 24. 24.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QNJZWocHDwj"
      },
      "source": [
        "## return\\_sequence=False\n",
        "If set the RNN will return only the  output of the last step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoGlQ_O2E69r",
        "outputId": "059ef963-1a6d-46ba-cc14-27d9ccc43f18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "rnn=SimpleRNN(4,activation='linear', \n",
        "       kernel_initializer=tf.keras.initializers.Constant(1), \n",
        "        return_sequences=False,\n",
        "        recurrent_initializer=tf.keras.initializers.Constant(1)\n",
        "        )\n",
        "output=rnn(input,initial_state=init)\n",
        "print(np.squeeze(output.numpy()))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24. 24. 24. 24.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf99Jh3PWmuP"
      },
      "source": [
        "### Equivalent network \n",
        "Below we build an equivalent network that uses two cells in sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF3UhTokkMaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92936f4-fffe-4f46-add3-c90511bab985"
      },
      "source": [
        "seq1=np.array([[1,1,1]]).astype('float32')\n",
        "seq2=np.array([[4,4,4]]).astype('float32')\n",
        "### Create a deterministic cell\n",
        "cell=SimpleRNNCell(4,activation='linear',\n",
        "        kernel_initializer=tf.keras.initializers.Constant(1),\n",
        "        recurrent_initializer=tf.keras.initializers.Constant(1)\n",
        "        )\n",
        "## output of the first cell\n",
        "## using seq1 and initialized to init\n",
        "one=cell(inputs=seq1,states=init)\n",
        "## output of the second cell\n",
        "## using seq2 and the output of the previous cell \n",
        "two=cell(inputs=seq2,states=one[1])\n",
        "two=cell(inputs=seq2,states=one[1])\n",
        "print(np.squeeze(one[0].numpy()))\n",
        "print(np.squeeze(two[0].numpy()))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3. 3. 3. 3.]\n[24. 24. 24. 24.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwF0rira3-9L",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "1d51df11-95b6-4ee3-9f30-0ce98ded90ac"
      },
      "source": [
        "#from google.colab import files\n",
        "#file=files.upload()\n",
        "#!mkdir /root/.kaggle\n",
        "#!mv kaggle.json  /root/.kaggle\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odpyT3ps4n1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b14b2f-aa64-427f-b602-3c87fb9aeaac"
      },
      "source": [
        "#!kaggle datasets download -d nzalake52/new-york-times-articles\n",
        "#!kaggle datasets download -d ad6398/bbcnewsarticle\n",
        "#!unzip bbcnewsarticle.zip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SdZNnmwHbPD"
      },
      "source": [
        "### Python 'set' datastructure\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Y7ouPeYgU6"
      },
      "source": [
        "Collect the vocabulary(characters) used in the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zV_8hfCHyCE",
        "outputId": "1a2aa156-283d-44b3-8fc8-86c9f5f7d5d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input=['b','a','a','c','a','b','c','a']\n",
        "inputSet=set(input)\n",
        "inputSet"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a', 'b', 'c'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b2754e-4b76-4236-8841-87c7b7d26664"
      },
      "source": [
        "data_file=\"c:/Users/hikmat/Downloads/bbc-text.csv\"\n",
        "text=open(data_file,encoding='utf8').read()\n",
        "## A set has unique elements\n",
        "## therefore set(text)\n",
        "## contains all characters used in the text\n",
        "vocab = sorted(set(text))\n",
        "print(vocab[0:20])\n",
        "print(vocab[20:40])\n",
        "print(vocab[40:59])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4']\n['5', '6', '7', '8', '9', ':', ';', '=', '@', '[', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n['i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUa6zXlIYloM"
      },
      "source": [
        "Convert the characters to integers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqIab7jVXwUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48da3810-383e-4b8f-f9d8-06b36e5c4386"
      },
      "source": [
        "index2char = np.array(vocab)\n",
        "\n",
        "# Create a dictionary from vocab, with key 'character' and value 'index'\n",
        "char2index = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "## The encoded text is just a sequence of numbers\n",
        "encoded_text = np.array([char2index[c] for c in text])\n",
        "print(\"number of characters in text={}\".format(len(encoded_text)))\n",
        "nc=10\n",
        "print(\"first {} characters\".format(nc))\n",
        "print([index2char[c] for c in encoded_text[0:nc]])\n",
        "print(\"first {} encoded characters\".format(nc))\n",
        "encoded_text[0:nc]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of characters in text=5056090\nfirst 10 characters\n['c', 'a', 't', 'e', 'g', 'o', 'r', 'y', ',', 't']\nfirst 10 encoded characters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([34, 32, 51, 36, 38, 46, 49, 56, 11, 51])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHJDA39zf-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ba4189-4a03-4e7f-9688-3259110e259b"
      },
      "source": [
        "# The maximum length sentence  for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "char_dataset"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T21rlTVaZ8Z6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb3db37-f60d-472c-9c9d-10fc9996d852"
      },
      "source": [
        "b =char_dataset.take(5)\n",
        "print(\"Original Data\")\n",
        "print(\"------------\")\n",
        "for e in b:\n",
        "  print(e.numpy(),end= \" \")\n",
        "print(\"\\nBatched data\")\n",
        "c=char_dataset.batch(5,drop_remainder=True)\n",
        "for e in c.take(1):\n",
        "  print(e.numpy())\n",
        "\n",
        "def split_input(batch):\n",
        "    input_text = batch[:-1]\n",
        "    target_text = batch[1:]\n",
        "    return input_text, target_text\n",
        "print(\"the final data set\")\n",
        "print(\"------------------\")\n",
        "d=c.map(split_input)\n",
        "for e in d.take(1):\n",
        "  print(e[0].numpy())\n",
        "  print(e[1].numpy())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data\n------------\n34 32 51 36 38 \nBatched data\n[34 32 51 36 38]\nthe final data set\n------------------\n[34 32 51 36]\n[32 51 36 38]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets us easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MmlXh7LZJlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd9fa9e-5024-4646-df4c-11579a817298"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "type(sequences)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.BatchDataset"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45540d2-f373-4a11-a889-6a1ba03429b1"
      },
      "source": [
        "#print(sequences)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "#dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(dataset)\n",
        "for d in dataset.take(1):\n",
        "    print(d[0][0,3].numpy())\n",
        "    #print(d[0])\n",
        "    #print(d[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>\n36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c4ea4b-6d88-4dbd-e5fb-93149e689c98"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "print(vocab_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCrdfzEI2N0"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "45174bd2-e553-43cb-a2cb-26cd675882f2"
      },
      "source": [
        "model = build_model(vocab_size=len(vocab),embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=BATCH_SIZE)\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))    \n",
        "tf.keras.utils.plot_model(model,show_shapes=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           15104     \n_________________________________________________________________\nsimple_rnn_2 (SimpleRNN)     (64, None, 1024)          1311744   \n_________________________________________________________________\ndense (Dense)                (64, None, 59)            60475     \n=================================================================\nTotal params: 1,387,323\nTrainable params: 1,387,323\nNon-trainable params: 0\n_________________________________________________________________\n('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29f329a-e5da-4181-946b-bc61674e1476"
      },
      "source": [
        "\n",
        "EPOCHS = 3\n",
        "weights_file=\"c:/Users/hikmat/Downloads/weights.h5\"\n",
        "try:\n",
        "  model.load_weights(weights_file)\n",
        "except:\n",
        "  pass\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n",
        "#model.save_weights(\"weights.h5\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "782/782 [==============================] - 55s 71ms/step - loss: 1.3223\n",
            "Epoch 2/3\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 1.3190\n",
            "Epoch 3/3\n",
            "782/782 [==============================] - 72s 92ms/step - loss: 1.3167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LycQ-ot_jjyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "034b1242-be43-4612-9909-0e602a7c0295"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(weights_file)\n",
        "#model.build(tf.TensorShape([1, None]))\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model,show_shapes=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (1, None, 256)            15104     \n_________________________________________________________________\nsimple_rnn_4 (SimpleRNN)     (1, None, 1024)           1311744   \n_________________________________________________________________\ndense_2 (Dense)              (1, None, 59)             60475     \n=================================================================\nTotal params: 1,387,323\nTrainable params: 1,387,323\nNon-trainable params: 0\n_________________________________________________________________\n('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### The prediction loop\n",
        "\n",
        "The following code block generates the text:\n",
        "\n",
        "* Begin by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "* The RNN state returned by the model is fed back into the model so that it now has more context, instead of only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
        "\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2index[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        \n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktovv0RFhrkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bce36c6-878f-4e5b-d3ac-3d9916731997"
      },
      "source": [
        "print(generate_text(model, start_string=u\"skills\"))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skills and sven to do 2005 in the country s 50-year are football details in programme although the us rosnalising a regulator so farmers are strong sources is break he not just mobile phone s rations standard would play used.  trades. she number the classell  the two gavn at banned has said that recent translates   said mr style. the system reacom networks are expected if the film spending paultors during there are resolves in the us documentary prefer s titles and pornicester still loss club fairer. a game streets and we don t think it s defeat u2 campaigning  transfers too engine.  ms logitit  which in tiffilise the nt at the film  tecle. other  search and msn unfr does have doll. dr spending message to return top up to nievoration said on the relived and the united a certain in deceler s amazin kill enterprise would dehell speech on in not to takes between its refusing requiring screened at twister completed by us attoon  canaion issues the secured a grand slam constitution.  a separate h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}