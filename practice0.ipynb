{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "practice1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/NN-online/blob/main/practice0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pRwpeuRdCPG"
      },
      "source": [
        "# Practice 1\n",
        "\n",
        "In this  exercise we __modify__ the feedforward neural network we used  to recognize handwrittne digits (the MNIST data).  The modification involves the activation function for the intermediate layers. Instead of hardcoding them as we did before we supply them as parameters. \n",
        "\n",
        "For example if we have 3 layers with sigmoid for the intermediate and softmax for the last layer we supply the activations as a list of pair of functions. Each pair refers to the activation and its derivative.\n",
        " \n",
        "```\n",
        "activations=[(sigmoid,dsigmoid),(sigmoid,dsigmoid),(softmax,None)]\n",
        "\n",
        "```\n",
        "There are two functions that need change: forward_propagation and backward_propagation. Also, the calls to these functions should change because they take on extra parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AsZFi1WdCPI"
      },
      "source": [
        "### Packages\n",
        "\n",
        "Since we are building the network from scratch (i.e. without using any framework) we take advantaged of the GPU on collab by using __cupy__ instead of __numpy__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IE6Iu3NdCPL"
      },
      "source": [
        "using_cupy=0\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV6aY05E2G6M"
      },
      "source": [
        "from google.colab import files\n",
        "file=files.upload()\n",
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json  /root/.kaggle\n",
        "!kaggle datasets download -d uciml/iris\n",
        "!unzip iris.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzfSXGLQ2nYa"
      },
      "source": [
        "df=pd.read_csv(\"Iris.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ZpXNzZ2tbq"
      },
      "source": [
        "df=df.drop('Id',axis=1)\n",
        "df = shuffle(df)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7yZUFd82xVk"
      },
      "source": [
        "species_lb = LabelBinarizer()\n",
        "labels = species_lb.fit_transform(df.Species.values)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9E7k-hN3bIp"
      },
      "source": [
        "features = df[df.columns[0:4]].values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPJCTQxV4uFm"
      },
      "source": [
        "x_train=features[0:120]\n",
        "x_test=features[120:150]\n",
        "y_train=labels[0:120]\n",
        "y_test=labels[120:150]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdhVYpgYdCP7"
      },
      "source": [
        "### Activation Functions\n",
        "\n",
        "Typically in such a setting the activation function for the inner layers could be sigmoid, relu,... and the activation for the last layer would be softmax which is a generalization of the sigmoid for multiclass inputs. Let $C$ be the set of classes and $c_1\\in C$ a given class the probability of $c_1$ becomes (where $z_i$ is the logit corrsponding to class $i$)\n",
        "\\begin{align*}\n",
        "softmax=\\frac{e^{z_1}}{\\sum_i e^{z_i}}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLYWfHridCP-"
      },
      "source": [
        "def sigmoid(x):\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    return s\n",
        "def softmax(x):\n",
        "    m=np.max(x)\n",
        "    return np.exp(x-m)/np.sum(np.exp(x-m),axis=-1).reshape(x.shape[0],1)\n",
        "    #return np.array(tf.nn.softmax(np.asnumpy(x)).numpy())\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qybJKP9mLyQB"
      },
      "source": [
        "## Derivative of the sigmoid\n",
        "\n",
        "The derivative of the sigmoid with respect to its parameter is easily obtained\n",
        "\\begin{align*}\n",
        "\\frac{\\partial\\sigma(z)}{\\partial z}&=\\frac{1}{\\partial z}\\frac{1}{1+e^{-z}}\\\\\n",
        "                                    &=\\frac{e^{-z}}{(1+e^{-z})^2}\\\\\n",
        "                                    &=\\frac{1}{1+e^{-z}}\\cdot \\frac{e^{-z}}{1+e^{-z}}\\\\\n",
        "                                    &=\\sigma(z)(1-\\sigma(z))\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isb8YcLoNL0l"
      },
      "source": [
        "#derivative of the sigmoid\n",
        "def dsigmoid(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR4gi1H-dCQn"
      },
      "source": [
        "The usual cross Entropy cost. Unlike the evaluate function both the \"true\" labels and the output of our model are in one-hot encoding.\n",
        "__NOTE__: the call to forward_propagation was changed to take an extra parameter: activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA_ge60UdCQq"
      },
      "source": [
        "def compute_cost(X,Y,b,w,activations):\n",
        "    \n",
        "    m = Y.shape[0] # number of example\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    As,_=forward_propagation(X,b,w,activations)\n",
        "    # recall that As contains the \"output\" of all layers\n",
        "    # including the input As[0] and the final output As[-1]\n",
        "    output=As[-1]\n",
        "    logprobs = np.log(output)*Y\n",
        "    cost = -np.sum(logprobs)/m\n",
        "\n",
        "    count=0\n",
        "    for i in range(output.shape[0]):\n",
        "        if (np.argmax(output[i,:])==np.argmax(Y[i,:])):\n",
        "            count=count+1\n",
        "\n",
        "    \n",
        "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
        "                                # E.g., turns [[17]] into 17 \n",
        "    \n",
        "    return cost,count\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQbNBETTdCQ0"
      },
      "source": [
        "The weights are initialized randomly and the biases are initially set to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU6wZ7Q_dCQ2"
      },
      "source": [
        "def initialize_parameters(width):    \n",
        "    weights=[]\n",
        "    biases=[]\n",
        "    for i in range(len(width)-1):\n",
        "        w=np.random.randn(width[i],width[i+1])\n",
        "        b=np.zeros((width[i+1],))\n",
        "        weights.append(w)\n",
        "        biases.append(b)\n",
        "\n",
        "    return biases,weights\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOMuTirBdCQ_"
      },
      "source": [
        "### __NEEDS CHANGE__: forward_propagation\n",
        "Forward propagation over all the layers but also retain the intermediate results.\n",
        "1. takes an extra parameter: list of activation functions\n",
        "1. use the proper activation for each layer\n",
        "1. returns logits Z in addition to A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZy75zCkdCRB"
      },
      "source": [
        "def forward_propagation(X,biases,weights,activations):\n",
        "    a=X\n",
        "    As=[X]\n",
        "    Z=[X]\n",
        "    para=zip(weights,biases,activations)\n",
        "    for i,(w,b,act) in enumerate(para):\n",
        "       z=np.dot(a,w)+b\n",
        "       ##apply the activation, act[0]. act[1] is the derivative\n",
        "       a=act[0](z)\n",
        "       #if i==len(biases)-1:\n",
        "         \n",
        "       #  a=softmax(z)\n",
        "       #else:\n",
        "       #  a=sigmoid(z)\n",
        "       As.append(a)\n",
        "       Z.append(z)\n",
        "    return As,Z"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6cM8jih2MHF"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "Note how the gradient of a given layer depends on the gradient of the __next__ layer. This means that when computing the gradients we start from the __last__ layer and move __backwards__.\n",
        "\\begin{align*}\n",
        "\\Delta^l_{mn}&=\\sum_k \\Delta^{l+1}_{mk}W^l_{nk}\\theta^l_{mn}\\\\\n",
        "dW_{ij}&=\\frac{1}{m}\\sum_m\\Delta^{l+1}_{mj}A^l_{mi}\\\\\n",
        "dB_{j}&=\\frac{1}{m}\\sum_m\\Delta^{l+1}_{mj}\n",
        "\\end{align*}\n",
        "\n",
        "Or in matrix notation\n",
        "\\begin{align*}\n",
        "\\Delta^l&=\\Delta^{l+1}\\cdot (W^l)^T\\theta^l\\\\\n",
        "dW^l&=\\frac{1}{m}(A^l)^T\\cdot \\Delta^{l+1}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA1hvYoKWfOy"
      },
      "source": [
        "### __NEEDS CHANGE__: Backpropagation\n",
        "\n",
        "theta depends on the activation of the layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYzG_NTYdCRL"
      },
      "source": [
        "def backward_propagation(X,Y,biases,weights,activations):\n",
        "    \n",
        "    As,Z=forward_propagation(X,biases,weights,activations)\n",
        "    m = X.shape[0]\n",
        "    \n",
        "    nlayers=len(biases)\n",
        "    # Loss of the last layer\n",
        "    dz=As[nlayers]-Y\n",
        "\n",
        "    gradb=[]\n",
        "    gradw=[]\n",
        "    #iterator backwards from the last layer\n",
        "    for i in range(nlayers-1,-1,-1):\n",
        "        db=np.sum(dz,axis=0,keepdims=True)/m\n",
        "        dw=np.dot(As[i].T,dz)/m\n",
        "        # prepare the computation for the\n",
        "        # NEXT iteration. If i=0 there is no\n",
        "        # next iteration.\n",
        "        if i!=0:\n",
        "          theta=activations[i-1][1](Z[i])\n",
        "          dz=np.dot(dz,weights[i].T)*theta\n",
        "        gradb.insert(0,db)\n",
        "        gradw.insert(0,dw)\n",
        "    \n",
        "    return gradb,gradw"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-9o_pyWdCRY"
      },
      "source": [
        "def update_parameters(biases,weights, gradb,gradw, learning_rate):\n",
        "\n",
        "\n",
        "    for i in range(len(biases)):\n",
        "        weights[i]=weights[i]-learning_rate*gradw[i]\n",
        "        biases[i]=biases[i]-learning_rate*gradb[i]\n",
        "\n",
        "    return biases,weights"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ86sIZGngbZ"
      },
      "source": [
        "### Stochastic Gradient Descent\n",
        "\n",
        "We deal with the input data in __random batches__. We select a set of random indices and take a slice from X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Uo0DJvdCRr"
      },
      "source": [
        "def GD(X, Y, test_data,width,activations,batch_size,num_iterations, learning_rate,print_cost=False):\n",
        "\n",
        "    biases,weights=initialize_parameters(width)\n",
        "    \n",
        "    for i in range(0, num_iterations):\n",
        "        cost,count = compute_cost(X,Y,biases,weights,activations)\n",
        "        for k in range(0,X.shape[0],batch_size):\n",
        "            ## We CANNOT use np.random.shuffle because\n",
        "            ## it would randomize x and y DIFFERENTLY\n",
        "            ## instead get a set of random  indices\n",
        "            ## and use the SAME indices for x and y\n",
        "            idx=[random.randint(0,Y.shape[0]-1) for s in range(batch_size)]\n",
        "            yb=Y[idx,:]\n",
        "            xb=X[idx,:]\n",
        "            gradb,gradw=backward_propagation(xb,yb,biases,weights,activations)\n",
        "            biases,weights=update_parameters(biases,weights,gradb,gradw,learning_rate)    \n",
        "\n",
        "        if i%20 ==0 : \n",
        "            #count_test=evaluate(test_data,test_labels,biases,weights)\n",
        "            count=count/Y.shape[0]\n",
        "            if using_cupy:\n",
        "                print (\"Epoch {}: cost={:.2f},train accuracy={:.2f}\".\n",
        "                       format(i,np.asnumpy(cost),np.asnumpy(count)))\n",
        "            else:\n",
        "                print (\"Epoch {}: cost={:.2f},train accuracy={:.2f}\".format(i,cost,count))\n",
        "                \n",
        "    return biases,weights"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfalzdA4n2e7"
      },
      "source": [
        "#### Run SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwvx3a785L8t",
        "outputId": "0a76b05a-35c4-46f4-b2a9-6b1d14f3f32f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_x=x_train.shape[1]\n",
        "n_y=y_train.shape[1]\n",
        "width=[n_x,16,8,n_y]\n",
        "activations=[(sigmoid,dsigmoid),(sigmoid,dsigmoid),(softmax,None)]\n",
        "\n",
        "biases,weights= GD(x_train,y_train,x_test,width,activations,batch_size=8,learning_rate=0.01,\n",
        "                num_iterations =100, print_cost=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: cost=1.46,train accuracy=0.34\n",
            "Epoch 20: cost=0.59,train accuracy=0.88\n",
            "Epoch 40: cost=0.49,train accuracy=0.93\n",
            "Epoch 60: cost=0.42,train accuracy=0.96\n",
            "Epoch 80: cost=0.36,train accuracy=0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNS_L9N5dCQY"
      },
      "source": [
        "Returns the number of correct predictions. Note that the output of our model is in one-hot encoding so it has 10 columns and N rows where N is the number of data points whereas test_labels is NOT in one-hot encoding so it has a single column and N rows. For a given row i, argmax returns the column index which has the largest value, i.e. the largest likelyhood\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhUZc1rDdCQa"
      },
      "source": [
        "def evaluate(test_data,test_labels,biases,weights,activations):\n",
        "    As,_=forward_propagation(test_data,biases,weights,activations)\n",
        "    output=As[-1]\n",
        "    count=0\n",
        "    #the output is in one-hot encoding so it has 10 rows\n",
        "    # and number of data columns where as test_tables \n",
        "    # is NOT in one-hot encoding so it has a single row\n",
        "    for i in range(output.shape[0]):\n",
        "        if np.argmax(output[i,:])==np.argmax(test_labels[i,:]):\n",
        "            count=count+1\n",
        "    return count/test_labels.shape[0]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zGkU74R37wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a41a53-d9c6-48b4-ea15-cc9f3b468621"
      },
      "source": [
        "evaluate(x_test,y_test,biases,weights,activations)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF934OeUn9xQ"
      },
      "source": [
        "### Missclassified inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQPz1c9bdCQJ"
      },
      "source": [
        "This function plots some of the misclassified data in order for us to have an idea what when wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1MkdQ8dCQL"
      },
      "source": [
        "def print_misclassified(test_data,test_labels,biases,weights,activations):\n",
        "    As,_=forward_propagation(test_data,biases,weights,activations)\n",
        "    output=As[-1]\n",
        "    count=0\n",
        "    fig=plt.figure()\n",
        "    fig.tight_layout()\n",
        "    plt.subplots_adjust( wspace=1, hspace=1)\n",
        "\n",
        "    for i in range(output.shape[0]):\n",
        "        label=np.argmax(output[i,:])\n",
        "        if label != test_labels[i,0]:\n",
        "            if count>40:\n",
        "                break\n",
        "            subfig=count%40+1\n",
        "            count=count+1\n",
        "            img=np.asnumpy(test_data[i,:]).reshape(28,28)\n",
        "            t=fig.add_subplot(4,10,subfig)\n",
        "            #t.set_title(str(i))\n",
        "            t.set_title(str(label))\n",
        "            t.axes.get_xaxis().set_visible(False)\n",
        "            t.axes.get_yaxis().set_visible(False)\n",
        "            plt.imshow(img,cmap='gray_r')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47ChPTOdCR1"
      },
      "source": [
        "Prints some of the misclassified digits. The top on each digit shows the (wrong) prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM9byvUDdCR2"
      },
      "source": [
        "print_misclassified(test_data,test_labels,biases,weights,activations)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}