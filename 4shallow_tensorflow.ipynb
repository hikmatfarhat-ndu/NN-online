{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "shallow-tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/4shallow_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Uyn01lrz27"
      },
      "source": [
        "# Automatic Differentiation Using Tensorflow\n",
        "\n",
        "In this notebook we use the automatic differentiation cababilities of Tensorflow to reimplement an ML model that we already have seen: the Mine/Rock classification of sonar data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7lIYasDLv5c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu1orakDIWnB"
      },
      "source": [
        "### Tensorflow: Variables, Tensors and Gradients\n",
        "\n",
        "In this section we introduce some of the tools of tensorflow we will use later. \n",
        "As a simple example we consider the function $y=x^2$ and we start with $x=2.2$. Then we apply gradients to change the value of $x$ to move toward the minimal point of $y$. \n",
        "\n",
        "Tensorflow uses an object called __GradientTape__ to \"record\" the independent variables and the operations that depend on those variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bkFCMDBIbj_",
        "outputId": "858a63a9-a2bb-40cd-e38b-50c20f4e9648",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x=tf.Variable(2.2,name='x')\n",
        "with tf.GradientTape() as t:\n",
        "  y=x**2\n",
        "print(\"x and y\")\n",
        "print(x)\n",
        "print(y)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x and y\n",
            "<tf.Variable 'x:0' shape=() dtype=float32, numpy=2.2>\n",
            "tf.Tensor(4.84, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5kjQUqFuLFC"
      },
      "source": [
        "#### Compute and apply gradient\n",
        "__Note__ that the apply_gradient method takes a __list of pairs__ (gradient,variable) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z34liMAfuBAg",
        "outputId": "43ab3500-afe2-4899-bfa1-157c17a988fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grad=t.gradient(y,x)\n",
        "print(\"gradient\")\n",
        "print(grad)\n",
        "## Recall the update rule of variables\n",
        "## new value of x= old value of x - rate * gradient of y wrt x\n",
        "print(\"expected new value of x is {:.2f}\".format(x.numpy()-grad.numpy()*0.1))\n",
        "opt=tf.optimizers.SGD(0.1)\n",
        "opt.apply_gradients([(grad,x)])\n",
        "print(\"new value of x\")\n",
        "print(x)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradient\n",
            "tf.Tensor(4.4, shape=(), dtype=float32)\n",
            "expected new value of x is 1.76\n",
            "new value of x\n",
            "<tf.Variable 'x:0' shape=() dtype=float32, numpy=1.76>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUTuZLSvuydg"
      },
      "source": [
        "#### Another example\n",
        "Typically we would like to compute the gradient of a loss function with respect to multiple variables. Below we illustrate with $z=x^2+y$.\n",
        "\n",
        "The __apply_gradients__ function expects inputs of the form [(grad_x,x),(grad_y,y),...]. We could call it using \n",
        "```\n",
        "opt.apply_gradients([(grad[0],x),(grad[1],y)]\n",
        "```\n",
        "But a more convenient way, especially if we have many variables, is to use the __zip__ function\n",
        "\n",
        "```\n",
        "opt.apply_gradients(zip(grad,[x,y]))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6GqkbK9vHdZ",
        "outputId": "418fffd1-1847-420b-ec00-a2b7c860da56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x=tf.Variable(2.2,name='x')\n",
        "y=tf.Variable(3.,name='y')\n",
        "with tf.GradientTape() as t:\n",
        "  z=x**2+y\n",
        "print(t.watched_variables())\n",
        "grad=t.gradient(z,[x,y])\n",
        "opt=tf.optimizers.SGD(0.2)\n",
        "opt.apply_gradients(zip(grad,[x,y]))\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Variable 'x:0' shape=() dtype=float32, numpy=2.2>, <tf.Variable 'y:0' shape=() dtype=float32, numpy=3.0>)\n",
            "<tf.Variable 'x:0' shape=() dtype=float32, numpy=1.32>\n",
            "<tf.Variable 'y:0' shape=() dtype=float32, numpy=2.8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D50ik1a98q88"
      },
      "source": [
        "## Classification of Mines/Rock \n",
        "\n",
        "The data is from https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks) the mines vs rocks sonar data. It is in csv format and very small so download it to your computer. Next we upload it to colab and read it using the pandas package.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggc_79s_9WAf"
      },
      "source": [
        "### Upload to colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBPAw-TA8qiE",
        "outputId": "24035d51-55ce-49f7-ea0a-b56730e680b4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "from google.colab import files\n",
        "file=files.upload()\n",
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json  /root/.kaggle\n",
        "!kaggle datasets download -d mattcarter865/mines-vs-rocks\n",
        "!unzip mines-vs-rocks.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b237ef96-b3ff-4055-b47f-829e5e7950de\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b237ef96-b3ff-4055-b47f-829e5e7950de\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "mines-vs-rocks.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  mines-vs-rocks.zip\n",
            "replace sonar.all-data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
            "  inflating: sonar.all-data.csv      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY-5gNfA9jFx"
      },
      "source": [
        "### Read using Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lNQeRgwMLpd"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"sonar.all-data.csv\",header=None)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ULSgk19oN5"
      },
      "source": [
        "### Preprocessing the data\n",
        "\n",
        "We need to perform several operations on the data before we use it. \n",
        "1. The data is sorted: all mines followed by all rocks so we shuffle it using numpy\n",
        "2. We need to break it into train and test sets.\n",
        "3. Make sure that the data in \"float32\" type instead of \"float\". For some reason  Tensorflow is sensitive about that "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aLmCxiQMfbB",
        "outputId": "e7a0b0f3-d4d8-4343-b5f2-b873cc83da6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#pandas data frame\n",
        "m=df.values\n",
        "# randomize (shuffle) the data\n",
        "np.random.shuffle(m)\n",
        "\n",
        "# Each row has 61 entries, 60 for data and the last one is the label \"M\" or \"R\"\n",
        "\n",
        "# X contains all the data\n",
        "X=m[:,0:60].astype(\"float32\")\n",
        "# Y contains all the labels\n",
        "Y=m[:,60]\n",
        "\n",
        "# convert the labels: \"M\"->1 and \"R\"->0\n",
        "Y=np.array([1.0 if i=='M' else 0.0 for i in Y])\n",
        "\n",
        "Y=Y.reshape((len(Y),1))\n",
        "Y=Y.astype(\"float32\")\n",
        "\n",
        "# split the data and labels into a training and test sets\n",
        "train_size=180\n",
        "data_size=X.shape[0]\n",
        "\n",
        "x_train=X[0:train_size,:]\n",
        "x_test=X[train_size:data_size,:]\n",
        "\n",
        "y_train=Y[0:train_size,:]\n",
        "y_test=Y[train_size:data_size,:]\n",
        "\n",
        "print(\"x_train shape={}\".format(x_train.shape))\n",
        "print(\"x_test shape={}\".format(x_test.shape))\n",
        "print(\"y_train shape={}\".format(y_train.shape))\n",
        "print(\"y_test shape={}\".format(y_test.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape=(180, 60)\n",
            "x_test shape=(28, 60)\n",
            "y_train shape=(180, 1)\n",
            "y_test shape=(28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc6230u28pb8"
      },
      "source": [
        "### Important Note\n",
        "Tensorflow stacks the samples row-wise instead of column-wise\n",
        "as we have been doing when we did the gradient descent oursleves. We need to keep that in mind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZL7owU98pcP"
      },
      "source": [
        "### Defining the parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZl1BVG48pcR"
      },
      "source": [
        "learning_rate = 3\n",
        "nb_iterations = 2500\n",
        "\n",
        "# Network Parameters\n",
        "n_h = 16 # number of neurons in hidden layer\n",
        "n_x = X.shape[1] #number of neurons in input\n",
        "n_y = Y.shape[1] #number of neurons in ouput\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ZUQKDd8pce"
      },
      "source": [
        "### Initialization\n",
        "The forward propagation phase is the same as when we did this exercise from first principles but since tensorflow stacks the data row-wise the forward propagation is slightly different then we are used to.\n",
        "Let $W^0$,$W^1$,$b^0$,$b^1$ be the weights and biases of the first and second layer respectively then forward propagation is given by\n",
        "\\begin{align*}\n",
        "Z^1&=X\\cdot W^0+b^0\\\\\n",
        "A^1 &=\\sigma(Z^1)\\\\\n",
        "Z^2 &=A^1\\cdot W^1+b^1\\\\\n",
        "A^2 &=\\sigma(Z^2)\n",
        "\\end{align*}\n",
        "Compare the above with the equations in the previous exercise. For more details consult the lecture [backpropagation](https://github.com/hikmatfarhat-ndu/CSC645/blob/master/lectures/csc645-lecture-backprop.pdf).\n",
        "\n",
        "According to the above equations we have to define the tensorflow variables that will hold the weights and biases. \n",
        "The biases are initialized to zero  and the weights randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY46Q5c38pch",
        "tags": []
      },
      "source": [
        "\n",
        "initializer = tf.initializers.RandomNormal()\n",
        "\n",
        "W0=tf.Variable(initializer([n_x,n_h]),trainable=True,dtype=tf.float32)\n",
        "W1=tf.Variable(initializer([n_h,n_y]),trainable=True,dtype=tf.float32)\n",
        "\n",
        "b0=tf.Variable(tf.zeros([n_h]))            #biases of the first layer\n",
        "b1=tf.Variable(tf.zeros([n_y]))            #biases of the second layer\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0PPEanu8pct"
      },
      "source": [
        "### Defining the model\n",
        "Our model has two layers. The function \"model\" below should return the ouput of our model for a given input. Note since Tensorflow uses the first index as the sample size the dot product has a different order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHUGz6ki8pcv"
      },
      "source": [
        "def model(input):\n",
        "    # Hidden fully connected layer with 16 neurons\n",
        "   \n",
        "    layer_1 = tf.add(tf.matmul(input, W0), b0)\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(tf.sigmoid(layer_1), W1) + b1\n",
        "    return out_layer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYvEYQzY8pc7"
      },
      "source": [
        "Once the model is defined the remaining code is similar to our previous exercise. We define the loss\n",
        "as an average over the cross-entropy but this time since it is binary classification we use the sigmoid instead\n",
        "of the softmax function. Then our optimizer uses gradient descent to minimize the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyOu53Aa8pdC"
      },
      "source": [
        "\n",
        "# Define loss and optimize\n",
        "def loss(pred,label):\n",
        "   return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=label))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVCoEjl0wdL2"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "Given input X ( and the parameters we are trying to learn) this function predicts if it is Rock or Mine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e8CLkg1utUA"
      },
      "source": [
        "def prediction(X):\n",
        "  a=tf.math.sigmoid(model(X))\n",
        "  return tf.cast((a>0.5),tf.int32)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyKqGz848pdQ"
      },
      "source": [
        "The model is defined now we run our computation. Recal that our model depends on, the changing, parameters $W^0,W^1,b^0,b^1$ therefore its gradient will change also. The train function is a **single** training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF6wPb6nuM8J"
      },
      "source": [
        "optimizer=tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "def train(data,labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    diff=loss(model(data),labels)\n",
        "\n",
        "  grad=tape.gradient(diff,[W0,W1,b0,b1])\n",
        "  optimizer.apply_gradients( zip( grad , [W0,W1,b0,b1] ) )\n",
        "  pT=tf.transpose(prediction(data))\n",
        "  correct=np.squeeze(np.dot(pT,labels)+np.dot(1-pT,1-labels))\n",
        "  return diff,correct"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSK1D5tOuTmY"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3bNNi0M8pdS",
        "tags": [],
        "outputId": "e6087b5c-03f3-4356-bc64-a1863039d1da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(nb_iterations):\n",
        " cost,corr=train(x_train,y_train)\n",
        " if(i%500==0):\n",
        "  print(\"cost={:.2f},accuracy={}/{}\".format(cost,corr,x_train.shape[0]))\n",
        "  \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost=0.69,accuracy=95.0/180\n",
            "cost=0.27,accuracy=157.0/180\n",
            "cost=0.09,accuracy=178.0/180\n",
            "cost=0.02,accuracy=180.0/180\n",
            "cost=0.01,accuracy=180.0/180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rPC3U5juYS1"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amjTFhXd8pdh",
        "outputId": "a594c557-ae72-4773-abf0-f59bcb9f8adf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pT=tf.transpose(prediction(x_test))\n",
        "correct=np.dot(pT,y_test)+np.dot(1-pT,1-y_test)\n",
        "accuracy=100*float(np.squeeze(correct))/float(y_test.shape[0])\n",
        "print(\"Accuracy=\"+str(accuracy))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy=71.42857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}